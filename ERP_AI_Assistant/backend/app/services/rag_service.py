"""
Zaawansowany service RAG z ulepszonymi technikami generowania odpowiedzi
Autor: Ulepszenia RAG dla ERP AI Assistant
Funkcje: Hybrid Search, Re-ranking, Query Expansion, Context Compression, Multi-step Reasoning
"""

import os
import re
import json
import logging
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict, Counter
import math

# NLP & ML
from sentence_transformers import SentenceTransformer, CrossEncoder
import anthropic
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Dodatkowe biblioteki dla NLP
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    logging.warning("SpaCy nie jest dostƒôpne - niekt√≥re funkcje NLP bƒôdƒÖ ograniczone")

logger = logging.getLogger(__name__)

# ============================================================================
# MODELS & DATA STRUCTURES
# ============================================================================

@dataclass
class QueryAnalysis:
    """Analiza zapytania u≈ºytkownika"""
    original_query: str
    processed_query: str
    expanded_queries: List[str]
    intent: str
    confidence: float
    keywords: List[str]
    entities: List[Dict[str, str]]
    complexity_score: float
    requires_multi_step: bool

@dataclass
class SearchResult:
    """Wynik wyszukiwania z dodatkowymi metrykami"""
    content: str
    metadata: Dict[str, Any]
    semantic_score: float
    bm25_score: float
    combined_score: float
    rerank_score: Optional[float] = None
    source: str = ""
    chunk_id: str = ""
    relevance_explanation: str = ""

@dataclass
class ContextBundle:
    """Pakiet kontekstu z metadanymi"""
    primary_context: str
    supporting_context: str
    sources: List[str]
    confidence_scores: List[float]
    context_length: int
    compression_ratio: float

@dataclass
class AdvancedResponse:
    """Rozszerzona odpowied≈∫ z metrykami jako≈õci"""
    answer: str
    confidence: float
    sources: List[str]
    reasoning_steps: List[str]
    validation_score: float
    context_relevance: float
    answer_completeness: float
    factual_consistency: float
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

# ============================================================================
# HYBRID SEARCH ENGINE
# ============================================================================

class HybridSearchEngine:
    """Silnik hybrydowego wyszukiwania (Semantic + BM25)"""
    
    def __init__(self, sentence_model: SentenceTransformer):
        self.sentence_model = sentence_model
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,
            stop_words=None,  # Zachowujemy polskie s≈Çowa kluczowe
            ngram_range=(1, 2),
            min_df=2
        )
        self.document_embeddings = None
        self.documents = []
        self.tfidf_matrix = None
        self.is_fitted = False
        
        # Polskie stop words dla ERP
        self.erp_stop_words = {
            'system', 'modu≈Ç', 'funkcja', 'opcja', 'parametr', 'pole', 
            'warto≈õƒá', 'dane', 'informacje', 'element', 'czƒô≈õƒá'
        }
    
    def fit(self, documents: List[str], metadatas: List[Dict]):
        """Trenuje modele na korpusie dokument√≥w"""
        logger.info(f"üîÑ Trenowanie modeli wyszukiwania na {len(documents)} dokumentach...")
        
        self.documents = documents
        self.metadatas = metadatas
        
        # Wygeneruj embeddingi semantyczne
        self.document_embeddings = self.sentence_model.encode(documents)
        
        # Wytrenuj TF-IDF
        processed_docs = [self._preprocess_for_bm25(doc) for doc in documents]
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_docs)
        
        self.is_fitted = True
        logger.info("‚úÖ Modele wyszukiwania wytrenowane")
    
    def _preprocess_for_bm25(self, text: str) -> str:
        """Przetwarzanie tekstu dla BM25"""
        # Podstawowe czyszczenie
        text = re.sub(r'[^\w\sƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Usu≈Ñ ERP stop words ale zachowaj wa≈ºne terminy biznesowe
        words = text.lower().split()
        filtered_words = [w for w in words if w not in self.erp_stop_words or len(w) > 8]
        
        return ' '.join(filtered_words)
    
    def search(self, query: str, top_k: int = 10, alpha: float = 0.7) -> List[SearchResult]:
        """Hybrydowe wyszukiwanie (alpha * semantic + (1-alpha) * BM25)"""
        if not self.is_fitted:
            raise ValueError("Model nie zosta≈Ç wytrenowany - wywo≈Çaj fit() najpierw")
        
        # Semantic search
        query_embedding = self.sentence_model.encode([query])
        semantic_scores = cosine_similarity(query_embedding, self.document_embeddings)[0]
        
        # BM25 search
        processed_query = self._preprocess_for_bm25(query)
        query_vector = self.tfidf_vectorizer.transform([processed_query])
        bm25_scores = cosine_similarity(query_vector, self.tfidf_matrix)[0]
        
        # Kombinuj wyniki
        combined_scores = alpha * semantic_scores + (1 - alpha) * bm25_scores
        
        # Sortuj i zwr√≥ƒá top-k
        top_indices = np.argsort(combined_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append(SearchResult(
                content=self.documents[idx],
                metadata=self.metadatas[idx],
                semantic_score=float(semantic_scores[idx]),
                bm25_score=float(bm25_scores[idx]),
                combined_score=float(combined_scores[idx]),
                source=self.metadatas[idx].get('source', 'unknown'),
                chunk_id=self.metadatas[idx].get('chunk_id', str(idx))
            ))
        
        return results

# ============================================================================
# QUERY PROCESSOR
# ============================================================================

class AdvancedQueryProcessor:
    """Zaawansowane przetwarzanie zapyta≈Ñ"""
    
    def __init__(self, claude_client: anthropic.Anthropic, sentence_model: SentenceTransformer):
        self.claude_client = claude_client
        self.sentence_model = sentence_model
        
        # NLP pipeline
        if SPACY_AVAILABLE:
            try:
                self.nlp = spacy.load("pl_core_news_sm")
            except OSError:
                self.nlp = None
                logger.warning("Model spaCy pl_core_news_sm nie jest dostƒôpny")
        else:
            self.nlp = None
    
    def analyze_query(self, query: str) -> QueryAnalysis:
        """Kompleksowa analiza zapytania"""
        
        # Podstawowa analiza
        processed_query = self._preprocess_query(query)
        keywords = self._extract_keywords(query)
        entities = self._extract_entities(query)
        
        # Rozszerzenie zapytania
        expanded_queries = self._expand_query(query)
        
        # Klasyfikacja intencji przez Claude
        intent_info = self._classify_intent_advanced(query)
        
        # Ocena z≈Ço≈ºono≈õci
        complexity_score = self._assess_complexity(query)
        requires_multi_step = complexity_score > 0.7
        
        return QueryAnalysis(
            original_query=query,
            processed_query=processed_query,
            expanded_queries=expanded_queries,
            intent=intent_info['intent'],
            confidence=intent_info['confidence'],
            keywords=keywords,
            entities=entities,
            complexity_score=complexity_score,
            requires_multi_step=requires_multi_step
        )
    
    def _preprocess_query(self, query: str) -> str:
        """Przetwarzanie zapytania"""
        # Normalizacja
        query = query.strip().lower()
        
        # Rozwiniƒôcie skr√≥t√≥w ERP
        erp_expansions = {
            'crm': 'customer relationship management',
            'erp': 'enterprise resource planning',
            'api': 'application programming interface',
            'hr': 'human resources',
            'bi': 'business intelligence'
        }
        
        for abbrev, expansion in erp_expansions.items():
            query = re.sub(rf'\b{abbrev}\b', f"{abbrev} {expansion}", query)
        
        return query
    
    def _extract_keywords(self, query: str) -> List[str]:
        """Ekstrakcja kluczowych s≈Ç√≥w"""
        if self.nlp:
            doc = self.nlp(query)
            keywords = [token.lemma_.lower() for token in doc 
                       if not token.is_stop and not token.is_punct and len(token.text) > 2]
        else:
            # Fallback bez spaCy
            words = re.findall(r'\b\w{3,}\b', query.lower())
            keywords = [w for w in words if w not in {'jak', 'czy', 'gdzie', 'kiedy', 'dlaczego'}]
        
        return list(set(keywords))
    
    def _extract_entities(self, query: str) -> List[Dict[str, str]]:
        """Ekstrakcja nazwanych entno≈õci"""
        entities = []
        
        if self.nlp:
            doc = self.nlp(query)
            for ent in doc.ents:
                entities.append({
                    'text': ent.text,
                    'label': ent.label_,
                    'start': ent.start_char,
                    'end': ent.end_char
                })
        
        # Dodatkowe regu≈Çy dla termin√≥w ERP
        erp_patterns = {
            r'\b(faktur[aey]|FV|faktury)\b': 'DOCUMENT',
            r'\b(kontrahent[a-z]*|klient[a-z]*|dostawc[a-z]*)\b': 'BUSINESS_ENTITY',
            r'\b(towar[a-z]*|produkt[a-z]*|us≈Çug[a-z]*)\b': 'PRODUCT',
            r'\b(magazyn[a-z]*|stan[a-z]*|zapas[a-z]*)\b': 'INVENTORY',
            r'\b(p≈Çatno≈õƒá|p≈Çatno≈õci|zap≈Çat[a-z]*)\b': 'PAYMENT'
        }
        
        for pattern, entity_type in erp_patterns.items():
            matches = re.finditer(pattern, query, re.IGNORECASE)
            for match in matches:
                entities.append({
                    'text': match.group(),
                    'label': entity_type,
                    'start': match.start(),
                    'end': match.end()
                })
        
        return entities
    
    def _expand_query(self, query: str) -> List[str]:
        """Rozszerzenie zapytania o synonimy i terminy pokrewne"""
        expansions = [query]  # Oryginalne zapytanie
        
        # Synonimy dla termin√≥w ERP
        synonyms = {
            'faktura': ['dokument sprzeda≈ºy', 'paragon', 'rachunek'],
            'klient': ['kontrahent', 'nabywca', 'odbiorca'],
            'towar': ['produkt', 'artyku≈Ç', 'rzecz'],
            'cena': ['koszt', 'warto≈õƒá', 'kwota'],
            'magazyn': ['warehouse', 'sk≈Çad', 'zapasy'],
            'sprzeda≈º': ['vendita', 'sales', 'handel'],
            'ksiƒôgowo≈õƒá': ['accounting', 'rachunkowo≈õƒá', 'finanse']
        }
        
        query_lower = query.lower()
        for term, term_synonyms in synonyms.items():
            if term in query_lower:
                for synonym in term_synonyms:
                    expanded = query_lower.replace(term, synonym)
                    if expanded != query_lower:
                        expansions.append(expanded)
        
        # Ograniczenie do 3 najlepszych rozszerze≈Ñ
        return expansions[:3]
    
    def _classify_intent_advanced(self, query: str) -> Dict[str, Any]:
        """Zaawansowana klasyfikacja intencji"""
        try:
            system_prompt = """Jeste≈õ ekspertem w systemach ERP. Przeanalizuj zapytanie i zwr√≥ƒá JSON z:
{
  "intent": "jedna z kategorii: konfiguracja|wdro≈ºenie|funkcjonalno≈õci|problem|dokumenty|integracje|finanse|magazyn|crm|api|raportowanie",
  "confidence": 0.0-1.0,
  "sub_intent": "bardziej szczeg√≥≈Çowa kategoria",
  "complexity": "simple|medium|complex",
  "action_type": "search|create|modify|delete|analyze|help"
}"""

            message = self.claude_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=200,
                system=system_prompt,
                messages=[{"role": "user", "content": f"Przeanalizuj zapytanie: {query}"}]
            )
            
            response_text = message.content[0].text.strip()
            try:
                result = json.loads(response_text)
                return {
                    'intent': result.get('intent', 'funkcjonalno≈õci'),
                    'confidence': float(result.get('confidence', 0.7)),
                    'sub_intent': result.get('sub_intent', ''),
                    'complexity': result.get('complexity', 'medium'),
                    'action_type': result.get('action_type', 'search')
                }
            except json.JSONDecodeError:
                return self._fallback_classification(query)
                
        except Exception as e:
            logger.warning(f"B≈ÇƒÖd klasyfikacji intencji: {e}")
            return self._fallback_classification(query)
    
    def _fallback_classification(self, query: str) -> Dict[str, Any]:
        """Fallback klasyfikacja na podstawie s≈Ç√≥w kluczowych"""
        query_lower = query.lower()
        
        intent_keywords = {
            'konfiguracja': ['konfigur', 'ustaw', 'paramter', 'opcj'],
            'wdro≈ºenie': ['wdro≈ºenie', 'implementacj', 'install'],
            'problem': ['b≈ÇƒÖd', 'error', 'problem', 'nie dzia≈Ça'],
            'api': ['api', 'integracj', 'endpoint', 'rest'],
            'finanse': ['finans', 'ksiƒôgow', 'p≈Çatno≈õƒá', 'faktur'],
            'magazyn': ['magazyn', 'stan', 'zapas', 'inwentarz'],
            'crm': ['crm', 'klient', 'sprzeda≈º', 'lead']
        }
        
        max_score = 0
        best_intent = 'funkcjonalno≈õci'
        
        for intent, keywords in intent_keywords.items():
            score = sum(1 for kw in keywords if kw in query_lower)
            if score > max_score:
                max_score = score
                best_intent = intent
        
        return {
            'intent': best_intent,
            'confidence': min(0.9, 0.5 + max_score * 0.1),
            'sub_intent': '',
            'complexity': 'medium',
            'action_type': 'search'
        }
    
    def _assess_complexity(self, query: str) -> float:
        """Ocena z≈Ço≈ºono≈õci zapytania (0.0 - 1.0)"""
        complexity_factors = [
            len(query.split()) > 10,  # D≈Çugie zapytanie
            '?' in query and len([q for q in query.split('?') if q.strip()]) > 1,  # Wiele pyta≈Ñ
            any(word in query.lower() for word in ['jak', 'dlaczego', 'por√≥wnaj', 'r√≥≈ºnica']),  # Z≈Ço≈ºone pytania
            any(word in query.lower() for word in ['i', 'oraz', 'a tak≈ºe', 'dodatkowo']),  # Z≈Ço≈ºone warunki
            len([w for w in query.split() if w.lower() in ['konfiguracja', 'integracja', 'implementacja']]) > 0  # Techniczne terminy
        ]
        
        return sum(complexity_factors) / len(complexity_factors)

# ============================================================================
# RE-RANKING SERVICE
# ============================================================================

class ReRankingService:
    """Service do ponownego rankowania wynik√≥w wyszukiwania"""
    
    def __init__(self, claude_client: anthropic.Anthropic):
        self.claude_client = claude_client
        
        # Cross-encoder model for re-ranking (optional)
        try:
            self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            self.has_cross_encoder = True
        except Exception:
            self.has_cross_encoder = False
            logger.warning("Cross-encoder nie jest dostƒôpny - u≈ºywam alternatywnego re-rankingu")
    
    def rerank_results(self, query: str, results: List[SearchResult], top_k: int = 5) -> List[SearchResult]:
        """Ponowne rankowanie wynik√≥w"""
        if not results:
            return results
        
        # Metoda 1: Cross-encoder (je≈õli dostƒôpny)
        if self.has_cross_encoder:
            results = self._rerank_with_cross_encoder(query, results)
        
        # Metoda 2: Claude-based relevance scoring
        results = self._rerank_with_claude(query, results[:10])  # Ograniczenie dla koszt√≥w
        
        # Metoda 3: Dodatkowe heurystyki
        results = self._apply_ranking_heuristics(query, results)
        
        return results[:top_k]
    
    def _rerank_with_cross_encoder(self, query: str, results: List[SearchResult]) -> List[SearchResult]:
        """Re-ranking z cross-encoder"""
        if len(results) <= 1:
            return results
        
        try:
            query_doc_pairs = [(query, result.content[:500]) for result in results]
            scores = self.cross_encoder.predict(query_doc_pairs)
            
            for i, score in enumerate(scores):
                results[i].rerank_score = float(score)
            
            # Sortuj po nowym score
            results.sort(key=lambda x: x.rerank_score or 0, reverse=True)
            
        except Exception as e:
            logger.warning(f"B≈ÇƒÖd cross-encoder re-ranking: {e}")
        
        return results
    
    def _rerank_with_claude(self, query: str, results: List[SearchResult]) -> List[SearchResult]:
        """Re-ranking z Claude do oceny relevance"""
        if len(results) <= 1:
            return results
        
        try:
            # Przygotuj kontekst dla Claude
            context_for_claude = ""
            for i, result in enumerate(results):
                context_for_claude += f"\n--- DOKUMENT {i+1} ---\n{result.content[:300]}...\n"
            
            system_prompt = """Jeste≈õ ekspertem ERP. Oce≈Ñ relevantno≈õƒá dokument√≥w do pytania u≈ºytkownika.
Zwr√≥ƒá JSON z listƒÖ: [{"doc_id": 1, "relevance": 0.0-1.0, "explanation": "kr√≥tkie uzasadnienie"}]
Sortuj od najbardziej relevantnych."""

            message = self.claude_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=500,
                system=system_prompt,
                messages=[{
                    "role": "user", 
                    "content": f"PYTANIE: {query}\n\nDOKUMENTY:{context_for_claude}"
                }]
            )
            
            response_text = message.content[0].text.strip()
            
            # Parsuj odpowied≈∫
            try:
                relevance_scores = json.loads(response_text)
                
                # Aktualizuj wyniki
                for item in relevance_scores:
                    doc_id = item.get('doc_id', 1) - 1  # 0-indexed
                    if 0 <= doc_id < len(results):
                        results[doc_id].rerank_score = item.get('relevance', 0.5)
                        results[doc_id].relevance_explanation = item.get('explanation', '')
                
                # Sortuj po relevance
                results.sort(key=lambda x: x.rerank_score or 0, reverse=True)
                
            except json.JSONDecodeError:
                logger.warning("Nie uda≈Ço siƒô sparsowaƒá odpowiedzi Claude do re-ranking")
            
        except Exception as e:
            logger.warning(f"B≈ÇƒÖd Claude re-ranking: {e}")
        
        return results
    
    def _apply_ranking_heuristics(self, query: str, results: List[SearchResult]) -> List[SearchResult]:
        """Zastosowanie dodatkowych heurystyk rankingu"""
        query_lower = query.lower()
        
        for result in results:
            bonus_score = 0.0
            
            # Bonus za dok≈Çadne dopasowanie termin√≥w
            query_terms = set(query_lower.split())
            content_terms = set(result.content.lower().split())
            exact_matches = len(query_terms.intersection(content_terms))
            bonus_score += exact_matches * 0.1
            
            # Bonus za kategorie dokument√≥w
            category = result.metadata.get('category', '')
            if category in ['api_funkcje', 'dokumenty', 'funkcje'] and 'api' in query_lower:
                bonus_score += 0.2
            elif category == 'ksiƒôgowo≈õƒá' and any(term in query_lower for term in ['finanse', 'ksiƒôgowo≈õƒá', 'faktura']):
                bonus_score += 0.2
            elif category == 'kontrahenci' and any(term in query_lower for term in ['klient', 'kontrahent']):
                bonus_score += 0.2
            
            # Bonus za d≈Çugo≈õƒá dokumentu (preferuj ≈õrednie d≈Çugo≈õci)
            content_length = len(result.content)
            if 200 <= content_length <= 1000:
                bonus_score += 0.1
            
            # Aktualizuj combined_score
            result.combined_score += bonus_score
        
        # Sortuj ponownie
        results.sort(key=lambda x: x.combined_score, reverse=True)
        
        return results

# ============================================================================
# CONTEXT COMPRESSOR
# ============================================================================

class ContextCompressor:
    """Kompresja kontekstu dla lepszego wykorzystania token√≥w"""
    
    def __init__(self, claude_client: anthropic.Anthropic, max_context_length: int = 2000):
        self.claude_client = claude_client
        self.max_context_length = max_context_length
    
    def compress_context(self, search_results: List[SearchResult], query: str) -> ContextBundle:
        """Kompresja kontekstu z zachowaniem najwa≈ºniejszych informacji"""
        
        if not search_results:
            return ContextBundle(
                primary_context="",
                supporting_context="",
                sources=[],
                confidence_scores=[],
                context_length=0,
                compression_ratio=1.0
            )
        
        # Krok 1: Usu≈Ñ duplikaty i podobne fragmenty
        unique_results = self._remove_duplicates(search_results)
        
        # Krok 2: WyciƒÖgnij kluczowe fragmenty
        key_excerpts = self._extract_key_excerpts(unique_results, query)
        
        # Krok 3: Sumuj podobne informacje
        summarized_context = self._summarize_similar_content(key_excerpts)
        
        # Krok 4: Skonstruuj final context
        primary_context, supporting_context = self._build_final_context(summarized_context)
        
        original_length = sum(len(r.content) for r in search_results)
        final_length = len(primary_context) + len(supporting_context)
        compression_ratio = final_length / original_length if original_length > 0 else 1.0
        
        return ContextBundle(
            primary_context=primary_context,
            supporting_context=supporting_context,
            sources=[r.source for r in unique_results],
            confidence_scores=[r.combined_score for r in unique_results],
            context_length=final_length,
            compression_ratio=compression_ratio
        )
    
    def _remove_duplicates(self, results: List[SearchResult]) -> List[SearchResult]:
        """Usuwa duplikaty i bardzo podobne fragmenty"""
        unique_results = []
        seen_content = set()
        
        for result in results:
            # Tw√≥rz hash z pierwszych 100 znak√≥w
            content_hash = hash(result.content[:100].lower().strip())
            
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append(result)
        
        return unique_results
    
    def _extract_key_excerpts(self, results: List[SearchResult], query: str) -> List[Dict[str, Any]]:
        """WyciƒÖga kluczowe fragmenty z dokument√≥w"""
        key_excerpts = []
        query_terms = set(query.lower().split())
        
        for result in results:
            sentences = re.split(r'[.!?]+', result.content)
            
            for sentence in sentences:
                if len(sentence.strip()) < 20:
                    continue
                
                sentence_terms = set(sentence.lower().split())
                relevance = len(query_terms.intersection(sentence_terms)) / len(query_terms) if query_terms else 0
                
                if relevance > 0.1:  # Pr√≥g relevantno≈õci
                    key_excerpts.append({
                        'text': sentence.strip(),
                        'source': result.source,
                        'relevance': relevance,
                        'original_score': result.combined_score
                    })
        
        # Sortuj po relevantno≈õci
        key_excerpts.sort(key=lambda x: x['relevance'], reverse=True)
        return key_excerpts[:20]  # Maksymalnie 20 fragment√≥w
    
    def _summarize_similar_content(self, excerpts: List[Dict[str, Any]]) -> str:
        """Podsumowuje podobne tre≈õci"""
        if not excerpts:
            return ""
        
        # Grupuj podobne fragmenty
        grouped_content = defaultdict(list)
        
        for excerpt in excerpts:
            # Prosta kategoryzacja na podstawie s≈Ç√≥w kluczowych
            key_words = set(w.lower() for w in excerpt['text'].split() if len(w) > 3)
            
            best_group = None
            max_overlap = 0
            
            for group_key, group_excerpts in grouped_content.items():
                if group_excerpts:
                    group_words = set(w.lower() for w in group_excerpts[0]['text'].split() if len(w) > 3)
                    overlap = len(key_words.intersection(group_words))
                    if overlap > max_overlap and overlap >= 2:
                        max_overlap = overlap
                        best_group = group_key
            
            if best_group:
                grouped_content[best_group].append(excerpt)
            else:
                new_key = f"group_{len(grouped_content)}"
                grouped_content[new_key] = [excerpt]
        
        # Tw√≥rz podsumowanie dla ka≈ºdej grupy
        summarized_parts = []
        for group_key, group_excerpts in grouped_content.items():
            if len(group_excerpts) == 1:
                summarized_parts.append(group_excerpts[0]['text'])
            else:
                # Kombinuj podobne fragmenty
                combined_text = " ".join([e['text'] for e in group_excerpts[:3]])
                if len(combined_text) > 300:
                    combined_text = combined_text[:300] + "..."
                summarized_parts.append(combined_text)
        
        return "\n\n".join(summarized_parts)
    
    def _build_final_context(self, summarized_content: str) -> Tuple[str, str]:
        """Buduje finalny kontekst podzielony na primary i supporting"""
        if len(summarized_content) <= self.max_context_length:
            return summarized_content, ""
        
        # Podziel na akapity
        paragraphs = summarized_content.split('\n\n')
        
        primary_context = ""
        supporting_context = ""
        current_length = 0
        
        # Priorytet dla pierwszych akapit√≥w (najbardziej relevantne)
        for i, paragraph in enumerate(paragraphs):
            if current_length + len(paragraph) <= self.max_context_length * 0.7:  # 70% dla primary
                primary_context += paragraph + "\n\n"
                current_length += len(paragraph)
            else:
                # Reszta idzie do supporting context
                remaining_space = self.max_context_length - current_length
                if remaining_space > 100:
                    supporting_context = "\n\n".join(paragraphs[i:])
                    if len(supporting_context) > remaining_space:
                        supporting_context = supporting_context[:remaining_space] + "..."
                break
        
        return primary_context.strip(), supporting_context.strip()

# ============================================================================
# RESPONSE GENERATOR
# ============================================================================

class AdvancedResponseGenerator:
    """Zaawansowany generator odpowiedzi z multi-step reasoning"""
    
    def __init__(self, claude_client: anthropic.Anthropic, config):
        self.claude_client = claude_client
        self.config = config
    
    def generate_response(self, query_analysis: QueryAnalysis, context_bundle: ContextBundle) -> AdvancedResponse:
        """Generuje zaawansowanƒÖ odpowied≈∫ z walidacjƒÖ jako≈õci"""
        
        if query_analysis.requires_multi_step:
            return self._generate_multi_step_response(query_analysis, context_bundle)
        else:
            return self._generate_single_step_response(query_analysis, context_bundle)
    
    def _generate_single_step_response(self, query_analysis: QueryAnalysis, context_bundle: ContextBundle) -> AdvancedResponse:
        """Generuje prostƒÖ odpowied≈∫"""
        
        # Przygotuj prompt
        system_prompt = self._build_system_prompt(query_analysis)
        user_prompt = self._build_user_prompt(query_analysis, context_bundle)
        
        # Generuj odpowied≈∫
        try:
            message = self.claude_client.messages.create(
                model=self.config.CLAUDE_MODEL,
                max_tokens=self.config.MAX_TOKENS,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            
            raw_answer = message.content[0].text.strip()
            
            # Walidacja i metryki
            validation_score = self._validate_response(raw_answer, query_analysis, context_bundle)
            
            return AdvancedResponse(
                answer=raw_answer,
                confidence=query_analysis.confidence,
                sources=context_bundle.sources,
                reasoning_steps=["Wyszukiwanie w bazie wiedzy", "Generowanie odpowiedzi"],
                validation_score=validation_score,
                context_relevance=self._calculate_context_relevance(query_analysis, context_bundle),
                answer_completeness=self._calculate_completeness(raw_answer, query_analysis),
                factual_consistency=validation_score
            )
            
        except Exception as e:
            logger.error(f"B≈ÇƒÖd generowania odpowiedzi: {e}")
            return self._generate_fallback_response(query_analysis)
    
    def _generate_multi_step_response(self, query_analysis: QueryAnalysis, context_bundle: ContextBundle) -> AdvancedResponse:
        """Generuje odpowied≈∫ wieloetapowƒÖ dla z≈Ço≈ºonych zapyta≈Ñ"""
        
        reasoning_steps = []
        intermediate_results = []
        
        # Krok 1: Dekompozycja zapytania
        decomposed_queries = self._decompose_complex_query(query_analysis.original_query)
        reasoning_steps.append(f"Podzieli≈Çem zapytanie na {len(decomposed_queries)} czƒô≈õci")
        
        # Krok 2: Odpowiedz na ka≈ºdƒÖ czƒô≈õƒá
        for i, sub_query in enumerate(decomposed_queries):
            sub_result = self._answer_sub_query(sub_query, context_bundle)
            intermediate_results.append(sub_result)
            reasoning_steps.append(f"Czƒô≈õƒá {i+1}: {sub_query[:50]}...")
        
        # Krok 3: Syntetyzuj finalnƒÖ odpowied≈∫
        final_answer = self._synthesize_multi_step_answer(query_analysis, intermediate_results, context_bundle)
        reasoning_steps.append("Po≈ÇƒÖczy≈Çem odpowiedzi w sp√≥jnƒÖ ca≈Ço≈õƒá")
        
        # Walidacja
        validation_score = self._validate_response(final_answer, query_analysis, context_bundle)
        
        return AdvancedResponse(
            answer=final_answer,
            confidence=min(query_analysis.confidence, validation_score),
            sources=context_bundle.sources,
            reasoning_steps=reasoning_steps,
            validation_score=validation_score,
            context_relevance=self._calculate_context_relevance(query_analysis, context_bundle),
            answer_completeness=self._calculate_completeness(final_answer, query_analysis),
            factual_consistency=validation_score
        )
    
    def _decompose_complex_query(self, query: str) -> List[str]:
        """Dekompozycja z≈Ço≈ºonego zapytania"""
        try:
            system_prompt = """Podziel z≈Ço≈ºone zapytanie o ERP na prostsze podpytania.
Zwr√≥ƒá listƒô JSON: ["podpytanie1", "podpytanie2", ...]
Maksymalnie 4 podpytania."""

            message = self.claude_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=300,
                system=system_prompt,
                messages=[{"role": "user", "content": f"Podziel zapytanie: {query}"}]
            )
            
            response_text = message.content[0].text.strip()
            try:
                sub_queries = json.loads(response_text)
                return sub_queries if isinstance(sub_queries, list) else [query]
            except json.JSONDecodeError:
                # Fallback - podziel po przecinkach lub "i"
                parts = re.split(r'[,;]|\bi\b|\boraz\b', query)
                return [part.strip() for part in parts if len(part.strip()) > 10][:4]
                
        except Exception as e:
            logger.warning(f"B≈ÇƒÖd dekompozycji zapytania: {e}")
            return [query]
    
    def _answer_sub_query(self, sub_query: str, context_bundle: ContextBundle) -> str:
        """Odpowiada na pojedyncze podpytanie"""
        try:
            relevant_context = self._find_relevant_context_for_subquery(sub_query, context_bundle)
            
            system_prompt = "Odpowiedz kr√≥tko i konkretnie na podpytanie na podstawie kontekstu ERP."
            
            message = self.claude_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=200,
                system=system_prompt,
                messages=[{
                    "role": "user", 
                    "content": f"KONTEKST: {relevant_context}\n\nPYTANIE: {sub_query}"
                }]
            )
            
            return message.content[0].text.strip()
            
        except Exception as e:
            logger.warning(f"B≈ÇƒÖd odpowiedzi na podpytanie: {e}")
            return f"Nie mogƒô odpowiedzieƒá na: {sub_query}"
    
    def _find_relevant_context_for_subquery(self, sub_query: str, context_bundle: ContextBundle) -> str:
        """Znajduje najlepszy kontekst dla podpytania"""
        # Prosta heurystyka - we≈∫ pierwszy fragment primary context
        context_parts = context_bundle.primary_context.split('\n\n')
        if context_parts:
            return context_parts[0][:500]
        return context_bundle.primary_context[:500]
    
    def _synthesize_multi_step_answer(self, query_analysis: QueryAnalysis, intermediate_results: List[str], context_bundle: ContextBundle) -> str:
        """Syntetyzuje finalnƒÖ odpowied≈∫ z wynik√≥w po≈õrednich"""
        try:
            combined_results = "\n".join([f"‚Ä¢ {result}" for result in intermediate_results])
            
            system_prompt = """Stw√≥rz sp√≥jnƒÖ, profesjonalnƒÖ odpowied≈∫ ≈ÇƒÖczƒÖc wyniki analizy wieloetapowej.
Zachowaj styl eksperta ERP o imieniu Marcin."""

            message = self.claude_client.messages.create(
                model=self.config.CLAUDE_MODEL,
                max_tokens=self.config.MAX_TOKENS,
                system=system_prompt,
                messages=[{
                    "role": "user", 
                    "content": f"""ORYGINALNE PYTANIE: {query_analysis.original_query}

WYNIKI ANALIZY:
{combined_results}

DODATKOWY KONTEKST: {context_bundle.primary_context[:500]}

Napisz sp√≥jnƒÖ odpowied≈∫ ≈ÇƒÖczƒÖcƒÖ wszystkie elementy."""
                }]
            )
            
            return message.content[0].text.strip()
            
        except Exception as e:
            logger.error(f"B≈ÇƒÖd syntezy odpowiedzi: {e}")
            return "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd podczas przetwarzania z≈Ço≈ºonego zapytania."
    
    def _build_system_prompt(self, query_analysis: QueryAnalysis) -> str:
        """Buduje system prompt dostosowany do intencji"""
        
        base_prompt = """Jeste≈õ Marcin, przyjazny ekspert ds. system√≥w ERP z wieloletnim do≈õwiadczeniem jako architekt IT.

Odpowiadaj naturalnie i p≈Çynnie:
- Wykorzystuj dostarczony kontekst z bazy wiedzy
- Pisz po polsku w spos√≥b rozmowny ale profesjonalny
- Bez nadmiernego formatowania markdown
- Jak prawdziwa rozmowa z ekspertem"""

        intent_specific = {
            'api': "Skup siƒô na technicznych aspektach API i integracji.",
            'konfiguracja': "Podaj konkretne kroki konfiguracji i ustawie≈Ñ.",
            'problem': "Zdiagnozuj problem i zaproponuj rozwiƒÖzania.",
            'wdro≈ºenie': "Om√≥w proces wdro≈ºenia i best practices.",
            'finanse': "Skup siƒô na aspektach finansowych i ksiƒôgowych.",
            'magazyn': "Koncentruj siƒô na logistyce i zarzƒÖdzaniu zapasami."
        }
        
        intent_addition = intent_specific.get(query_analysis.intent, "")
        if intent_addition:
            base_prompt += f"\n\n{intent_addition}"
        
        base_prompt += "\n\nNa ko≈Ñcu dodaj dyskretnie: üîó Marcin - Architekt IT | üéØ [confidence]% | üìö ≈πr√≥d≈Ça: [lista]"
        
        return base_prompt
    
    def _build_user_prompt(self, query_analysis: QueryAnalysis, context_bundle: ContextBundle) -> str:
        """Buduje user prompt"""
        
        prompt_parts = []
        
        if context_bundle.primary_context:
            prompt_parts.append(f"KONTEKST G≈Å√ìWNY:\n{context_bundle.primary_context}")
        
        if context_bundle.supporting_context:
            prompt_parts.append(f"KONTEKST DODATKOWY:\n{context_bundle.supporting_context}")
        
        prompt_parts.extend([
            f"INTENCJA: {query_analysis.intent}",
            f"S≈ÅOWA KLUCZOWE: {', '.join(query_analysis.keywords)}",
            f"PYTANIE: {query_analysis.original_query}"
        ])
        
        return "\n\n".join(prompt_parts)
    
    def _validate_response(self, answer: str, query_analysis: QueryAnalysis, context_bundle: ContextBundle) -> float:
        """Waliduje jako≈õƒá odpowiedzi (0.0-1.0)"""
        
        validation_factors = []
        
        # 1. D≈Çugo≈õƒá odpowiedzi
        length_score = min(1.0, len(answer) / 200) if len(answer) > 50 else 0.3
        validation_factors.append(length_score)
        
        # 2. Obecno≈õƒá s≈Ç√≥w kluczowych z zapytania
        query_words = set(query_analysis.keywords)
        answer_words = set(answer.lower().split())
        keyword_overlap = len(query_words.intersection(answer_words)) / len(query_words) if query_words else 0.5
        validation_factors.append(keyword_overlap)
        
        # 3. U≈ºycie kontekstu (sprawd≈∫ czy odpowied≈∫ korzysta z dostarczonych informacji)
        if context_bundle.primary_context:
            context_words = set(context_bundle.primary_context.lower().split())
            context_usage = len(answer_words.intersection(context_words)) / len(context_words) if context_words else 0.3
            validation_factors.append(min(1.0, context_usage * 5))  # Scale up
        else:
            validation_factors.append(0.5)
        
        # 4. Struktura odpowiedzi (sprawd≈∫ czy ma sens)
        structure_score = 0.8 if len(re.findall(r'[.!?]', answer)) >= 2 else 0.5
        validation_factors.append(structure_score)
        
        # 5. Specyfika ERP (sprawd≈∫ czy zawiera terminy bran≈ºowe)
        erp_terms = {'erp', 'system', 'modu≈Ç', 'konfiguracja', 'funkcja', 'dokument', 'proces'}
        erp_usage = len(erp_terms.intersection(answer_words)) / len(erp_terms)
        validation_factors.append(erp_usage)
        
        return sum(validation_factors) / len(validation_factors)
    
    def _calculate_context_relevance(self, query_analysis: QueryAnalysis, context_bundle: ContextBundle) -> float:
        """Oblicza relevantno≈õƒá kontekstu do zapytania"""
        if not context_bundle.confidence_scores:
            return 0.5
        
        # ≈örednia z confidence scores
        avg_confidence = sum(context_bundle.confidence_scores) / len(context_bundle.confidence_scores)
        
        # Bonus za kompresjƒô (lepsze wykorzystanie kontekstu)
        compression_bonus = min(0.2, (1 - context_bundle.compression_ratio) * 0.5)
        
        return min(1.0, avg_confidence + compression_bonus)
    
    def _calculate_completeness(self, answer: str, query_analysis: QueryAnalysis) -> float:
        """Oblicza kompletno≈õƒá odpowiedzi"""
        
        completeness_factors = []
        
        # 1. D≈Çugo≈õƒá vs z≈Ço≈ºono≈õƒá zapytania
        expected_length = 100 + (query_analysis.complexity_score * 200)
        length_ratio = min(1.0, len(answer) / expected_length)
        completeness_factors.append(length_ratio)
        
        # 2. Pokrycie aspekt√≥w zapytania
        if query_analysis.entities:
            entity_texts = [e['text'].lower() for e in query_analysis.entities]
            answer_lower = answer.lower()
            entity_coverage = sum(1 for entity in entity_texts if entity in answer_lower) / len(entity_texts)
            completeness_factors.append(entity_coverage)
        else:
            completeness_factors.append(0.7)
        
        # 3. Struktura odpowiedzi dla z≈Ço≈ºonych zapyta≈Ñ
        if query_analysis.complexity_score > 0.7:
            # Z≈Ço≈ºone zapytania powinny mieƒá strukturƒô
            has_structure = bool(re.search(r'\n|[0-9]+\.|‚Ä¢|‚Üí', answer))
            completeness_factors.append(1.0 if has_structure else 0.6)
        else:
            completeness_factors.append(0.8)
        
        return sum(completeness_factors) / len(completeness_factors)
    
    def _generate_fallback_response(self, query_analysis: QueryAnalysis) -> AdvancedResponse:
        """Generuje fallback odpowied≈∫ w przypadku b≈Çƒôdu"""
        return AdvancedResponse(
            answer="Przepraszam, wystƒÖpi≈Ç problem z przetwarzaniem Twojego zapytania. Spr√≥buj ponownie lub zadaj pytanie w inny spos√≥b.",
            confidence=0.3,
            sources=[],
            reasoning_steps=["B≈ÇƒÖd systemu"],
            validation_score=0.3,
            context_relevance=0.0,
            answer_completeness=0.5,
            factual_consistency=0.3
        )

# ============================================================================
# MAIN ADVANCED RAG SERVICE
# ============================================================================

class AdvancedRAGService:
    """G≈Ç√≥wny service zaawansowanego RAG"""
    
    def __init__(self, config, ai_service, vector_service):
        self.config = config
        self.ai_service = ai_service
        self.vector_service = vector_service
        
        # Inicjalizuj komponenty
        self.hybrid_search = HybridSearchEngine(ai_service.sentence_model)
        self.query_processor = AdvancedQueryProcessor(ai_service.claude_client, ai_service.sentence_model)
        self.reranker = ReRankingService(ai_service.claude_client)
        self.context_compressor = ContextCompressor(ai_service.claude_client)
        self.response_generator = AdvancedResponseGenerator(ai_service.claude_client, config)
        
        self.is_initialized = False
    
    def initialize_with_documents(self, documents: List[str], metadatas: List[Dict]):
        """Inicjalizuje system z dokumentami"""
        logger.info("üöÄ Inicjalizacja zaawansowanego RAG...")
        
        # Wytrenuj hybrid search
        self.hybrid_search.fit(documents, metadatas)
        
        self.is_initialized = True
        logger.info("‚úÖ Zaawansowany RAG gotowy")
    
    def process_query(self, query: str, session_id: str = "default") -> AdvancedResponse:
        """G≈Ç√≥wne przetwarzanie zapytania"""
        
        if not self.is_initialized:
            logger.warning("RAG nie zosta≈Ç zainicjalizowany")
            return self._fallback_response(query)
        
        try:
            # Krok 1: Analiza zapytania
            query_analysis = self.query_processor.analyze_query(query)
            logger.info(f"üìù Analiza: intencja={query_analysis.intent}, z≈Ço≈ºono≈õƒá={query_analysis.complexity_score:.2f}")
            
            # Krok 2: Hybrydowe wyszukiwanie
            search_results = self.hybrid_search.search(query_analysis.processed_query, top_k=15)
            
            # Sprawd≈∫ czy sƒÖ wyniki
            if not search_results:
                logger.warning("Brak wynik√≥w wyszukiwania")
                return self._fallback_response(query)
            
            # Krok 3: Re-ranking
            reranked_results = self.reranker.rerank_results(query, search_results, top_k=8)
            
            # Krok 4: Kompresja kontekstu
            context_bundle = self.context_compressor.compress_context(reranked_results, query)
            
            # Krok 5: Generowanie odpowiedzi
            response = self.response_generator.generate_response(query_analysis, context_bundle)
            
            logger.info(f"‚úÖ Odpowied≈∫: confidence={response.confidence:.2f}, validation={response.validation_score:.2f}")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd w zaawansowanym RAG: {e}")
            return self._fallback_response(query)
    
    def _fallback_response(self, query: str) -> AdvancedResponse:
        """Fallback odpowied≈∫"""
        return AdvancedResponse(
            answer="Przepraszam, wystƒÖpi≈Ç problem z systemem RAG. Spr√≥buj ponownie.",
            confidence=0.3,
            sources=[],
            reasoning_steps=["B≈ÇƒÖd systemu"],
            validation_score=0.3,
            context_relevance=0.0,
            answer_completeness=0.5,
            factual_consistency=0.3
        )
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """Zwraca metryki systemu"""
        return {
            "initialized": self.is_initialized,
            "hybrid_search_ready": self.hybrid_search.is_fitted,
            "components": {
                "query_processor": bool(self.query_processor),
                "reranker": bool(self.reranker),
                "context_compressor": bool(self.context_compressor),
                "response_generator": bool(self.response_generator)
            }
        }
